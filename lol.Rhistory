text = tm_map(text,stripWhitespace)              #to remove white spaces
```
- stopwords() contains a list of irrelevant words in English which usually doesnot help the algorithm learn about the reviews.
- stemDocument helps in getting the root in each word so that we can train the model using just one version of the same words leading to much better accuracy.
- removing white spaces becomes important at this point as we have removed and transformed some of the words. It will bring uniformity and make sure that only the words are retained, no empty spaces before or after them.
# N grams Tokenizer
- We will now tokenize the words from the individual reviews and make coulmn of all the individual unique words. This is done using a Document term Matrix or DTM.
- But single words in themselves maynot be as powerful in suggeting the review. We also take phrases - 2 word & 3 word phrases for better prominence of the terms indicating the sentiment of reviews.
eg.s can be - "best movie ever" , "worst movie" etc.
```{r}
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
```
- We create a BigramTokenizer and a TrigramTokenizer containing 2 word
and 3 word phrases respectively using *Rweka* package.
- Creating the DTMs for 1-grams, 2-grams & 3-grams
```{r}
dtm1 = DocumentTermMatrix(text)
dtm1 = removeSparseTerms(dtm1,0.95)
## this is dtm for 1 words. we will find the more important words and just use those
dtm2 = DocumentTermMatrix(text, control = list(tokenize = BigramTokenizer))
dtm2 = removeSparseTerms(dtm2,0.99)
## similarly, dtm of 2 words
dtm3 = DocumentTermMatrix(text, control = list(tokenize = TrigramTokenizer))
dtm3 = removeSparseTerms(dtm3,0.999)
##dtm for 3 words
```
# Seeing the final DTMs
- We will also see the most frequently occuring 1 word , 2-word & 3-word phrases.
```{r}
dataset1 = as.data.frame(as.matrix(dtm1))
dataset2 = as.data.frame(as.matrix(dtm2))
dataset3 = as.data.frame(as.matrix(dtm3))
#for dtm1
dataset_counts1 = as.data.frame(colSums(dataset1))
dataset_counts1$word = rownames(dataset_counts1)
colnames(dataset_counts1) = c("count","word")
dataset_counts1 = dataset_counts1[c(2,1)]
dataset_counts1 = dataset_counts1 %>% arrange(-count)
#for dtm2
dataset_counts2 = as.data.frame(colSums(dataset2))
dataset_counts2$word = rownames(dataset_counts2)
colnames(dataset_counts2) = c("count","word")
dataset_counts2 = dataset_counts2[c(2,1)]
dataset_counts2 = dataset_counts2 %>% arrange(-count)
#for dtm3
dataset_counts3 = as.data.frame(colSums(dataset3))
dataset_counts3$word = rownames(dataset_counts3)
colnames(dataset_counts3) = c("count","word")
dataset_counts3 = dataset_counts3[c(2,1)]
dataset_counts3= dataset_counts3 %>% arrange(-count)
head(dataset_counts1,20)
head(dataset_counts2,20)
head(dataset_counts3,20)
```
#Model fitting
```{r}
#Pre processing
final_dataset_words = bind_rows(dataset_counts1,dataset_counts2,dataset_counts3)
final_dataset = as.data.frame(cbind(dataset1,dataset2,dataset3))
dataset_train = final_dataset[1:10241,]
dataset_test = final_dataset[10242:14630,]
```
- We build a random forest classifier
- Next, we also see which are the most important words accoring to the model. We have a look at top 200 such words.
# Model fitting - Logistic Regression
```{r}
dataset_train$y_pred = word_train$sentiment
dataset_train$y_pred = as.factor(dataset_train$y_pred)
m1 = glm(formula = y_pred ~.,
data = dataset_train,
family = 'binomial')
pred = predict(m1,newdata = dataset_test)
m1 = glm(formula = y_pred ~,
data = dataset_train,
family = 'binomial')
m1 = glm(formula = y_pred ~.,
data = dataset_train,
family = 'binomial')
dataset_train
install.packages("text2vec")
install.packages("data.table")
install.packages("magrittr")
tb <- tibble(id = dat$tweet_id, text = dat$text,
sentminet = dat$airline_sentiment)
library(tibble)
tb <- tibble(id = dat$tweet_id, text = dat$text,
sentminet = dat$airline_sentiment)
tb
tb1 <- tb[1:10241
,]
tb2 <- tb[10242:14630
,]
library(text2vec)
library(data.table)
library(magrittr)
data("movie_review")
setDT(movie_review)
setkey(movie_review, id)
View(data("movie_review"))
View(movie_review)
data("tb")
setDT(tb)
setkey(tb, id)
set.seed(2017L)
all_ids = tb$id
train_ids = sample(all_ids, 10241)
test_ids = setdiff(all_ids, train_ids)
train = tb[J(train_ids)]
test = tb[J(test_ids)]
train
tb <- tibble(id = dat$tweet_id, text = dat$text,
sentiment = dat$airline_sentiment)
setDT(tb)
setkey(tb, id)
set.seed(2017L)
all_ids = tb$id
train_ids = sample(all_ids, 10241)
test_ids = setdiff(all_ids, train_ids)
train = tb[J(train_ids)]
test = tb[J(test_ids)]
train
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(train$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = train$id,
progressbar = FALSE)
vocab = create_vocabulary(it_train)
tb
tb$text <- as.character(tb$text)
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(train$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = train$id,
progressbar = FALSE)
vocab = create_vocabulary(it_train)
setDT(tb)
setkey(tb, id)
set.seed(2017L)
all_ids = tb$id
train_ids = sample(all_ids, 10241)
test_ids = setdiff(all_ids, train_ids)
train = tb[J(train_ids)]
test = tb[J(test_ids)]
train
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(train$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = train$id,
progressbar = FALSE)
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
dim(dtm_train)
identical(rownames(dtm_train), train$id)
train_ids
setDT(tb)
setkey(tb, id)
set.seed(2017L)
all_ids = tb$id
train_ids = sample(all_ids, 10000)
test_ids = setdiff(all_ids, train_ids)
train = tb[J(train_ids)]
test = tb[J(test_ids)]
train
setDT(tb)
setkey(tb, id)
set.seed(2017L)
all_ids = tb$id
train_ids = sample(all_ids, 10241)
test_ids = setdiff(all_ids, train_ids)
train = tb[J(train_ids)]
test = tb[J(test_ids)]
train
install.packages("glmnet")
library(glmnet)
NFOLDS = 4
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']],
family = 'binomial',
# L1 penalty
alpha = 1,
# interested in the area under ROC curve
type.measure = "auc",
# 5-fold cross-validation
nfolds = NFOLDS,
# high value is less accurate, but has faster training
thresh = 1e-3,
# again lower number of iterations for faster training
maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))
library(glmnet)
NFOLDS = 4
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']],
family = 'multinomial',
# L1 penalty
alpha = 1,
# interested in the area under ROC curve
type.measure = "auc",
# 5-fold cross-validation
nfolds = NFOLDS,
# high value is less accurate, but has faster training
thresh = 1e-3,
# again lower number of iterations for faster training
maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))
plot(glmnet_classifier)
it_test = test$review %>%
prep_fun %>% tok_fun %>%
# turn off progressbar because it won't look nice in rmd
itoken(ids = test$id, progressbar = FALSE)
dtm_test = create_dtm(it_test, vectorizer)
preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)
View(tb)
View(test_ids)
View(all_ids)
train_ids = tb[1:10241,1]
View(train_ids)
test_ids = tb[10242:14640]
test_ids
test_ids = 'NULL
''
''
test_ids = 'NULL'
test_ids
test_ids = tb[10242:14640, 1]
test_ids
train = tb[J(train_ids)]
test = tb[J(test_ids)]
train
train
test = tb[10242:14640,]
train = tb[1:10241,]
train
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(train$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = train$id,
progressbar = FALSE)
vocab = create_vocabulary(it_train)
train_tokens = train$review %>%
prep_fun %>%
tok_fun
it_train = itoken(train_tokens,
ids = train$id,
# turn off progressbar because it won't look nice in rmd
progressbar = FALSE)
vocab = create_vocabulary(it_train)
vocab
vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
dim(dtm_train)
identical(rownames(dtm_train), train$id)
View(train$id)
rownames(dtm_train)
identical(rownames(dtm_train), train$id)
library(glmnet)
NFOLDS = 4
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']],
family = 'binomial',
# L1 penalty
alpha = 1,
# interested in the area under ROC curve
type.measure = "auc",
# 5-fold cross-validation
nfolds = NFOLDS,
# high value is less accurate, but has faster training
thresh = 1e-3,
# again lower number of iterations for faster training
maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))
library(glmnet)
NFOLDS = 4
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']],
family = 'multinomial',
# L1 penalty
alpha = 1,
# interested in the area under ROC curve
type.measure = "auc",
# 5-fold cross-validation
nfolds = NFOLDS,
# high value is less accurate, but has faster training
thresh = 1e-3,
# again lower number of iterations for faster training
maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))
it_test = test$review %>%
prep_fun %>% tok_fun %>%
# turn off progressbar because it won't look nice in rmd
itoken(ids = test$id, progressbar = FALSE)
dtm_test = create_dtm(it_test, vectorizer)
preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)
full = bind_rows(train, test)
library(dplyr)
library(tm)
library(knitr)
library(RWeka)
library(randomForest)
full = bind_rows(train, test)
dim(word_train)
dim(word_test)
str(full)
full$text= gsub(full$text, pattern = '<br />', replacement = ' ')
text = VCorpus(VectorSource(full$review))        # Creating a Corpus of reviews
text = tm_map(text,content_transformer(tolower))  # Converting to lower case
text = tm_map(text,removeNumbers)                # Removing numbers
#as.character(text[[1]])
text = tm_map(text,removePunctuation)
text = tm_map(text,removeWords,stopwords())      #to remove common words
text = tm_map(text,stemDocument)                 #to convert words back to root words.
text = tm_map(text,stripWhitespace)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
View(text)
dtm1 = DocumentTermMatrix(text)
dtm1 = removeSparseTerms(dtm1,0.95)
## this is dtm for 1 words. we will find the more important words and just use those
dtm2 = DocumentTermMatrix(text, control = list(tokenize = BigramTokenizer))
dtm2 = removeSparseTerms(dtm2,0.99)
## similarly, dtm of 2 words
dtm3 = DocumentTermMatrix(text, control = list(tokenize = TrigramTokenizer))
dtm3 = removeSparseTerms(dtm3,0.999)
##dtm for 3 words
dataset1 = as.data.frame(as.matrix(dtm1))
dataset2 = as.data.frame(as.matrix(dtm2))
dataset3 = as.data.frame(as.matrix(dtm3))
#for dtm1
dataset_counts1 = as.data.frame(colSums(dataset1))
dataset_counts1$word = rownames(dataset_counts1)
colnames(dataset_counts1) = c("count","word")
dataset_counts1 = dataset_counts1[c(2,1)]
dataset_counts1 = dataset_counts1 %>% arrange(-count)
#for dtm2
dataset_counts2 = as.data.frame(colSums(dataset2))
dataset_counts2$word = rownames(dataset_counts2)
colnames(dataset_counts2) = c("count","word")
dataset_counts2 = dataset_counts2[c(2,1)]
dataset_counts2 = dataset_counts2 %>% arrange(-count)
#for dtm3
dataset_counts3 = as.data.frame(colSums(dataset3))
dataset_counts3$word = rownames(dataset_counts3)
colnames(dataset_counts3) = c("count","word")
dataset_counts3 = dataset_counts3[c(2,1)]
dataset_counts3= dataset_counts3 %>% arrange(-count)
head(dataset_counts1,20)
head(dataset_counts2,20)
head(dataset_counts3,20)
final_dataset_words = bind_rows(dataset_counts1,dataset_counts2,dataset_counts3)
final_dataset = as.data.frame(cbind(dataset1,dataset2,dataset3))
dataset_train = final_dataset[1:10241,]
dataset_test = final_dataset[102421:14640,]
dataset_train$y_pred = word_train$sentiment
dataset_train$y_pred = as.factor(dataset_train$y_pred)
m1 = glm(formula = y_pred ~.,
data = dataset_train,
family = 'binomial')
pred = predict(m1,newdata = dataset_test)
pred
View(pred)
train
test
full
View(full)
full$text
full$text = gsub(full$text, pattern = '<br />', replacement = ' ')
full
head(full)
text
text = VCorpus(VectorSource(full$text))
text = tm_map(text,content_transformer(tolower))  # Converting to lower case
text = tm_map(text,removeNumbers)                # Removing numbers
#as.character(text[[1]])
text = tm_map(text,removePunctuation)
text = tm_map(text,removeWords,stopwords())      #to remove common words
text = tm_map(text,stemDocument)                 #to convert words back to root words.
text = tm_map(text,stripWhitespace)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
dtm1 = DocumentTermMatrix(text)
dtm1 = removeSparseTerms(dtm1,0.95)
## this is dtm for 1 words. we will find the more important words and just use those
dtm2 = DocumentTermMatrix(text, control = list(tokenize = BigramTokenizer))
dtm2 = removeSparseTerms(dtm2,0.99)
## similarly, dtm of 2 words
dtm3 = DocumentTermMatrix(text, control = list(tokenize = TrigramTokenizer))
dtm3 = removeSparseTerms(dtm3,0.999)
##dtm for 3 words
dataset1 = as.data.frame(as.matrix(dtm1))
dataset2 = as.data.frame(as.matrix(dtm2))
dataset3 = as.data.frame(as.matrix(dtm3))
#for dtm1
dataset_counts1 = as.data.frame(colSums(dataset1))
dataset_counts1$word = rownames(dataset_counts1)
colnames(dataset_counts1) = c("count","word")
dataset_counts1 = dataset_counts1[c(2,1)]
dataset_counts1 = dataset_counts1 %>% arrange(-count)
#for dtm2
dataset_counts2 = as.data.frame(colSums(dataset2))
dataset_counts2$word = rownames(dataset_counts2)
colnames(dataset_counts2) = c("count","word")
dataset_counts2 = dataset_counts2[c(2,1)]
dataset_counts2 = dataset_counts2 %>% arrange(-count)
#for dtm3
dataset_counts3 = as.data.frame(colSums(dataset3))
dataset_counts3$word = rownames(dataset_counts3)
colnames(dataset_counts3) = c("count","word")
dataset_counts3 = dataset_counts3[c(2,1)]
dataset_counts3= dataset_counts3 %>% arrange(-count)
head(dataset_counts1,20)
head(dataset_counts2,20)
head(dataset_counts3,20)
final_dataset_words = bind_rows(dataset_counts1,dataset_counts2,dataset_counts3)
final_dataset = as.data.frame(cbind(dataset1,dataset2,dataset3))
dataset_train = final_dataset[1:10241,]
dataset_test = final_dataset[10242:14640,]
dataset_train$y_pred = word_train$sentiment
dataset_train$y_pred = as.factor(dataset_train$y_pred)
m1 = glm(formula = y_pred ~.,
data = dataset_train,
family = 'binomial')
pred = predict(m1,newdata = dataset_test)
View(pred)
acc <- mean(pred)
acc
dataset_train$y_pred = train$sentiment
dataset_train$y_pred = as.factor(dataset_train$y_pred)
m1 = glm(formula = y_pred ~.,
data = dataset_train,
family = 'binomial')
pred = predict(m1,newdata = dataset_test)
View(pred)
acc <- mean(pred)
acc
word_train
full = bind_rows(train, test)
myCorpus <- Corpus(VectorSource(full$text))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myStopwords <- c(setdiff(stopwords('english'), c("r", "big")), "use", "see", "used", "via", "amp")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
text <- myCorpus
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
dtm1 = DocumentTermMatrix(text)
dtm1 = removeSparseTerms(dtm1,0.95)
## this is dtm for 1 words. we will find the more important words and just use those
dtm2 = DocumentTermMatrix(text, control = list(tokenize = BigramTokenizer))
dtm2 = removeSparseTerms(dtm2,0.99)
## similarly, dtm of 2 words
dtm3 = DocumentTermMatrix(text, control = list(tokenize = TrigramTokenizer))
dtm3 = removeSparseTerms(dtm3,0.999)
##dtm for 3 words
dataset1 = as.data.frame(as.matrix(dtm1))
dataset2 = as.data.frame(as.matrix(dtm2))
dataset3 = as.data.frame(as.matrix(dtm3))
#for dtm1
dataset_counts1 = as.data.frame(colSums(dataset1))
dataset_counts1$word = rownames(dataset_counts1)
colnames(dataset_counts1) = c("count","word")
dataset_counts1 = dataset_counts1[c(2,1)]
dataset_counts1 = dataset_counts1 %>% arrange(-count)
#for dtm2
dataset_counts2 = as.data.frame(colSums(dataset2))
dataset_counts2$word = rownames(dataset_counts2)
colnames(dataset_counts2) = c("count","word")
dataset_counts2 = dataset_counts2[c(2,1)]
dataset_counts2 = dataset_counts2 %>% arrange(-count)
#for dtm3
dataset_counts3 = as.data.frame(colSums(dataset3))
dataset_counts3$word = rownames(dataset_counts3)
colnames(dataset_counts3) = c("count","word")
dataset_counts3 = dataset_counts3[c(2,1)]
dataset_counts3= dataset_counts3 %>% arrange(-count)
head(dataset_counts1,20)
head(dataset_counts2,20)
head(dataset_counts3,20)
final_dataset_words = bind_rows(dataset_counts1,dataset_counts2,dataset_counts3)
final_dataset = as.data.frame(cbind(dataset1,dataset2,dataset3))
dataset_train = final_dataset[1:10241,]
dataset_test = final_dataset[10242:14640,]
dataset_train$y_pred = train$sentiment
dataset_train$y_pred = as.factor(dataset_train$y_pred)
m1 = glm(formula = y_pred ~.,
data = dataset_train,
family = 'binomial')
pred = predict(m1,newdata = dataset_test)
View(pred)
acc <- mean(pred)
acc
savehistory("~/IT 485/lol.Rhistory")
